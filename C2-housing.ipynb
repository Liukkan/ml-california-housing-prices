{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler,  StandardScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "sklearn.set_config(display='diagram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    tarball_path = Path(\"./datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"./datasets/housing/housing.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vistazo rapido a la estructura de datos\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploracion Inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada Registro es un distrito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacion general de los datos\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Categoricas\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos Datos son por distritos y no por casas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Numerica - Se ignora los valores nulos\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma de las features\n",
    "housing.hist(bins=100, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear conjuto de pruebas y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separacion de conjuntos sin estratificacion\n",
    "# X_train, X_test = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que una separacion de conjuntos sea mas efectiva hacemos una separacion estratificada segun una variable importante en este caso seleccionamos el `median_income` pero tenemos que transformarla a una variable categorica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['median_income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformacion de la feature \"median_income\" a una feature categorica\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos la distribucion\n",
    "housing[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0,grid=True)\n",
    "plt.xlabel\n",
    "plt.ylabel(\"Number of Districts\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=0.2, random_state=42, stratify=housing[\"income_cat\"])\n",
    "# Luego de usar esta columna la eliminamos\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una copia del dataset de entrenamiento para analizarlo\n",
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA y Visualizacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos Geograficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar Datos Geograficos\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2, grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, s=housing[\"population\"]/100, label=\"population\",\n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True, legend=True, sharex=False, figsize=(10,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-numeric columns for correlation matrix\n",
    "corr_matrix = housing.drop(\"ocean_proximity\", axis=1).corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentar con combinaciones de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuesvos atributos\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"] / housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"] / housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"] / housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.drop(\"ocean_proximity\", axis=1).corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # Guardamos los datos de entrenamiento sin la variable a predecir\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() # Guardamos la variable a predecir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Numericas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a limpiar los datos nulos con un imputer\n",
    "impute = SimpleImputer(strategy=\"median\") # Creamos un imputer con la estrategia de reemplazo de valores nulos por la mediana\n",
    "housing_num = housing.select_dtypes(include=[np.number]) # Seleccionamos solo las columnas numericas\n",
    "\n",
    "impute.fit(housing_num) # Ajustamos el imputer a los datos\n",
    "X = impute.transform(housing_num) # Transformamos los datos, Genera una salida de tipo numpy array\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) # Convertimos el array a un dataframe de pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributos de texto o categoticos\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder(sparse_output=False) # Creamos un encoder para las variables categoricas\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat) # Transformamos las variables categoricas a numericas\n",
    "type(housing_cat_1hot) # Salida de tipo scipy sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_ # Categorias de las variables categoricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escalado de caracteristicas y Transformacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizacion de los datos\n",
    "min_max_scaler = MinMaxScaler() # Creamos un escalador para normalizar los datos\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num) # Normalizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler() # Creamos un escalador para estandarizar los datos\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num) # Estandarizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp) # Creamos un transformador para aplicar logaritmo a los datos, El argumento inverse_func es opcional\n",
    "log_pop = log_transformer.transform(housing[[\"population\"]]) # Aplicamos el logaritmo a la columna \"population\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador personalizado \n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom transformer that computes the similarity of each sample to the cluster centers\n",
    "    using the Radial Basis Function (RBF) kernel.\n",
    "\n",
    "    Parameters:\n",
    "    n_clusters (int): The number of clusters to form as well as the number of centroids to generate.\n",
    "    gamma (float): The gamma parameter for the RBF kernel.\n",
    "    random_state (int, RandomState instance or None): Determines random number generation for centroid initialization.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Compute k-means clustering.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like or sparse matrix, shape=(n_samples, n_features)): Training instances to cluster.\n",
    "        y (ignored): Not used, present here for API consistency by convention.\n",
    "        sample_weight (array-like, shape (n_samples,), optional): The weights for each observation in X.\n",
    "\n",
    "        Returns:\n",
    "        self: Fitted estimator.\n",
    "        \"\"\"\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X to a cluster-distance space.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like or sparse matrix, shape=(n_samples, n_features)): New data to transform.\n",
    "\n",
    "        Returns:\n",
    "        array, shape (n_samples, n_clusters): X transformed in the new space.\n",
    "        \"\"\"\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        \"\"\"\n",
    "        Get output feature names for transformation.\n",
    "\n",
    "        Parameters:\n",
    "        names (array-like of str or None): Input feature names.\n",
    "\n",
    "        Returns:\n",
    "        list of str: Transformed feature names.\n",
    "        \"\"\"\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")), # Imputamos los valores nulos\n",
    "    (\"standardize\", StandardScaler()) # Estandarizamos los datos\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    (\"Impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"Encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num) # Aplicamos el pipeline a los datos numericos\n",
    "housing_num_prepared[:2].round(2) # Mostramos los primeros 2 registros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(data=housing_num_prepared,\n",
    "                                       columns=num_pipeline.get_feature_names_out(),\n",
    "                                       index=housing_num.index)\n",
    "df_housing_num_prepared.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num.columns) # Lista de atributos numericos\n",
    "cat_attribs = list(housing_cat.columns) # Lista de atributos categoricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de los datos con ColumnTransformer\n",
    "preprocessing_name = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])\n",
    "preprocessing_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer(\n",
    "    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "    (cat_pipeline, make_column_selector(dtype_include=object))\n",
    "    )\n",
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared = preprocessing.fit_transform(housing) # Aplicamos el preprocesamiento a los datos\n",
    "print(type(housing_prepared)) # Salida de tipo numpy array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformacion Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "    \"\"\"\n",
    "    Calcula la razón entre la primera y la segunda columna de un array numpy.\n",
    "\n",
    "    Parámetros:\n",
    "    X (numpy.ndarray): Un array numpy de dos o más columnas.\n",
    "\n",
    "    Retorna:\n",
    "    numpy.ndarray: Un array numpy con la razón entre la primera y la segunda columna.\n",
    "    \"\"\"\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "\n",
    "def ratio_name(function_transformer, features_names_in):\n",
    "    \"\"\"\n",
    "    Genera el nombre de la nueva característica creada por el transformador de función.\n",
    "\n",
    "    Parámetros:\n",
    "    function_transformer (FunctionTransformer): El transformador de función que se está utilizando.\n",
    "    features_names_in (list): Lista de nombres de las características de entrada.\n",
    "\n",
    "    Retorna:\n",
    "    list: Una lista con el nombre de la nueva característica.\n",
    "    \"\"\"\n",
    "    return [\"ratio\"]\n",
    "\n",
    "\n",
    "def ratio_pipeline():\n",
    "    \"\"\"\n",
    "    Crea un pipeline para calcular la razón entre la primera y la segunda columna,\n",
    "    imputar valores nulos con la mediana y estandarizar los datos.\n",
    "\n",
    "    Retorna:\n",
    "    sklearn.pipeline.Pipeline: Un pipeline de scikit-learn.\n",
    "    \"\"\"\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1.0, random_state=42)\n",
    "\n",
    "default_num_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_final = ColumnTransformer([\n",
    "    (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "    (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "    (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "    (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\", \"households\", \"median_income\"]),\n",
    "    (\"geo\", cluster_simil, [\"latitude\", \"longitude\"]),\n",
    "    (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "],\n",
    "remainder=default_num_pipeline,\n",
    "verbose=True\n",
    ")\n",
    "preprocessing_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_final = preprocessing_final.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_final.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(housing_prepared_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = make_pipeline(preprocessing_final, LinearRegression())\n",
    "lin_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = lin_reg.predict(housing)\n",
    "housing_predictions[:5].round(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = make_pipeline(preprocessing_final, DecisionTreeRegressor(random_state=42))\n",
    "tree_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictions = tree_reg.predict(housing)\n",
    "housing_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse = root_mean_squared_error(housing_labels, housing_predictions)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmses = cross_val_score(estimator=tree_reg, X=housing, y=housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "# Since sklearn's cross_val_score returns negative values for error metrics,\n",
    "# we need to negate them to get the actual RMSE values\n",
    "pd.Series(-tree_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_reg = make_pipeline(preprocessing_final, RandomForestRegressor(random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_rmses = -cross_val_score(estimator=forest_reg, X=housing, y=housing_labels,\n",
    "                                scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(forest_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda exhaustiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([ # Creamos un pipeline con los pasos de preprocesamiento y el modelo\n",
    "    (\"preprocessing\", preprocessing_final),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'preprocessing__geo__n_clusters': [5, 8, 10], # Number of clusters on the geo transformer on the preprocessing step\n",
    "        'random_forest__max_features': [4, 5, 9] # Number of features to consider when looking for the best split\n",
    "    },\n",
    "    {\n",
    "        'preprocessing__geo__n_clusters': [10, 15], # number of clusters on the geo transformer on the preprocessing step\n",
    "        'random_forest__max_features': [6, 8, 10] # number of features to consider when looking for the best split\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=full_pipeline, param_grid=param_grid,\n",
    "                           scoring=\"neg_root_mean_squared_error\", cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "cv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    'preprocessing__geo__n_clusters': [np.random.randint(low=3, high=50)],\n",
    "    'random_forest__max_features': [np.random.randint(low=2, high=20)]\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=50, cv=3,\n",
    "    scoring='neg_root_mean_squared_error', random_state=42, verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rnd_search.best_estimator_ # Guardamos el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = final_model[\"random_forest\"].feature_importances_ # Obtenemos las importancias de las features\n",
    "features_importances.round(2) # Mostramos las importancias de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(features_importances, final_model[\"preprocessing\"].get_feature_names_out()), reverse=True) # Mostramos las importancias de las features ordenadas de mayor a menor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar con el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1) # Guardamos los datos de test sin la variable a predecir\n",
    "y_test = strat_test_set[\"median_house_value\"].copy() # Guardamos la variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse = root_mean_squared_error(y_test, final_prediction) # Calculamos el RMSE de la prediccion\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.95\n",
    "squared_erros = (final_prediction - y_test) ** 2 # Calculamos el error cuadratico\n",
    "np.sqrt(stats.t.interval(\n",
    "    confidence=confidence,\n",
    "    df=len(squared_erros)-1,\n",
    "    loc=np.mean(squared_erros),\n",
    "    scale=stats.sem(squared_erros)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanzar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(final_model, \"my_california_housing_model.pkl\") # Guardamos el modelo en un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_realoaded = joblib.load(\"my_california_housing_model.pkl\") # Cargamos el modelo desde el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = X_test.iloc[:5] # Cargamos los primeros 5 registros del conjunto de test\n",
    "predictions = final_model_realoaded.predict(new_data) # Hacemos la prediccion con el modelo recargado\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
